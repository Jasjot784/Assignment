{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6081380,"sourceType":"datasetVersion","datasetId":3481581},{"sourceId":6100914,"sourceType":"datasetVersion","datasetId":3494603}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Name: Jasjot Singh\n 2022EET2105\n\nTask:\n### Task 1 | GPT-2 Model & Checkpoints (20 Points)\n\n---\n\nStart by implementing the `GPT2-small` model (with 125 million parameters) using Python and PyTorch. Make sure you touch upon the key aspects of the model like multi-head self-attention mechanism, feed-forward networks and positional encoding.\n\nKey points:\n\n- Follow the original GPT-2 design of using both token and positional embeddings.\n- Implement the transformer layers with multi-head self-attention and point-wise feed-forward network.\n- You're required to abstain from using pre-built transformer libraries.\n\nRefer to the GPT-2 paper's architecture descriptions in Sections 1 and 2 for further help. ([GPT-2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)). Additionally, a great resource could be Andrej Karpathy’s [nanogpt](https://github.com/karpathy/nanoGPT) repository and the [makemore](https://youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&feature=shared) series.\n\nTo validate your implementation, load the original GPT-2 125M model checkpoints and run a sample prediction.","metadata":{"id":"D2Ig2qQpqQOF"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters to manage the model parameters\nbatch_size = 16\nblock_size = 32\nmax_iters = 5000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 64\nn_head = 4\nn_layer = 4\ndropout = 0.0\n\n\n# ------------\n\ntorch.manual_seed(1337)\n\n#downlaoding the data set for manual training\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n\n#data preprocessing\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test dataset splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = BigramLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2qO7dOGAU9oL","outputId":"4087e8d2-16ae-41e9-d67e-f2867b7d2531"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"--2023-11-30 13:04:42--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n\nHTTP request sent, awaiting response... 200 OK\n\nLength: 1115394 (1.1M) [text/plain]\n\nSaving to: ‘input.txt’\n\n\n\ninput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n\n\n\n2023-11-30 13:04:42 (46.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n\n\n\n0.209729 M parameters\n\nstep 0: train loss 4.4116, val loss 4.4022\n\nstep 100: train loss 2.6568, val loss 2.6670\n\nstep 200: train loss 2.5091, val loss 2.5060\n\nstep 300: train loss 2.4199, val loss 2.4337\n\nstep 400: train loss 2.3500, val loss 2.3563\n\nstep 500: train loss 2.2961, val loss 2.3126\n\nstep 600: train loss 2.2408, val loss 2.2501\n\nstep 700: train loss 2.2053, val loss 2.2187\n\nstep 800: train loss 2.1636, val loss 2.1870\n\nstep 900: train loss 2.1226, val loss 2.1483\n\nstep 1000: train loss 2.1017, val loss 2.1283\n\nstep 1100: train loss 2.0683, val loss 2.1174\n\nstep 1200: train loss 2.0376, val loss 2.0798\n\nstep 1300: train loss 2.0256, val loss 2.0645\n\nstep 1400: train loss 1.9919, val loss 2.0362\n\nstep 1500: train loss 1.9696, val loss 2.0304\n\nstep 1600: train loss 1.9625, val loss 2.0470\n\nstep 1700: train loss 1.9402, val loss 2.0119\n\nstep 1800: train loss 1.9085, val loss 1.9957\n\nstep 1900: train loss 1.9080, val loss 1.9869\n\nstep 2000: train loss 1.8834, val loss 1.9941\n\nstep 2100: train loss 1.8727, val loss 1.9758\n\nstep 2200: train loss 1.8585, val loss 1.9622\n\nstep 2300: train loss 1.8537, val loss 1.9503\n\nstep 2400: train loss 1.8419, val loss 1.9424\n\nstep 2500: train loss 1.8153, val loss 1.9407\n\nstep 2600: train loss 1.8267, val loss 1.9374\n\nstep 2700: train loss 1.8126, val loss 1.9344\n\nstep 2800: train loss 1.8054, val loss 1.9230\n\nstep 2900: train loss 1.8045, val loss 1.9339\n\nstep 3000: train loss 1.7963, val loss 1.9243\n\nstep 3100: train loss 1.7691, val loss 1.9208\n\nstep 3200: train loss 1.7506, val loss 1.9092\n\nstep 3300: train loss 1.7548, val loss 1.9038\n\nstep 3400: train loss 1.7582, val loss 1.8960\n\nstep 3500: train loss 1.7376, val loss 1.8934\n\nstep 3600: train loss 1.7232, val loss 1.8888\n\nstep 3700: train loss 1.7280, val loss 1.8814\n\nstep 3800: train loss 1.7221, val loss 1.8951\n\nstep 3900: train loss 1.7228, val loss 1.8789\n\nstep 4000: train loss 1.7168, val loss 1.8635\n\nstep 4100: train loss 1.7168, val loss 1.8798\n\nstep 4200: train loss 1.7088, val loss 1.8672\n\nstep 4300: train loss 1.6995, val loss 1.8501\n\nstep 4400: train loss 1.7096, val loss 1.8686\n\nstep 4500: train loss 1.6907, val loss 1.8546\n\nstep 4600: train loss 1.6868, val loss 1.8348\n\nstep 4700: train loss 1.6786, val loss 1.8346\n\nstep 4800: train loss 1.6659, val loss 1.8445\n\nstep 4900: train loss 1.6711, val loss 1.8384\n\nstep 4999: train loss 1.6630, val loss 1.8230\n\n\n\nROMEO:\n\nBut you far you\n\nmy swap with thus; come hath I uD\n\nIf sleemition of where's granded\n\nOf their of tout the gortune upwon alond, liege man to is Iell this surpe\n\nAnd than sleue thus mind, his by blow,\n\nVirdty toward butied, Ditire spresiss with thou some not.\n\n\n\nLORIO:\n\nI am part\n\nBut thou sging them but\n\nshat secondes morry thou sovore.\n\n\n\nISABUS:\n\nWhat art sade but hither, thange e'en,\n\nProtes as kingle me; an your tords whom are Ineal.\n\n\n\nMENENIUS:\n\nBut little sweet, hom, foust cerfort;\n\nWinth hing diend enirs' tompy beds sick ways!\n\nWhat curforself this grace. Won, passes us.\n\n\n\nBUCKINGHABY MARD:\n\nMether star: keep it any head which\n\nHe tall devioly that, out that confer old.\n\nOur thy dears time.\n\nNay, the fragoly, pair, of new\n\nmy father, my lip Backnoward:\n\nGod therring for respide\n\nWhat colvery, teminelyord, I mast,\n\nWhile us that such differs I'll that confect I come,\n\nBut; man.\n\n\n\nVOLUMNIO:\n\nOntread confail with me. Humser dipporbried answeraw is codal one,\n\nOnjestion, not or cheavess ensty with.\n\n\n\nGLOUCESTER:\n\n\n\nHENRY Mess to Lies?\n\nStand and these beguare youf stile that than war\n\noffity are, I usquesch\n\nFrown movhapty not duke with you addom\n\ngrack prowd--lost\n\nBut but they worse is senst my crunne undolier. But, beauts pruntaly; I stoll'ct my nor Murder, I sot, though who speak\n\nYour bout told-man rathing if anyshal\n\nepitence, tirre no the said he's,\n\nAndis frultifs. what his lide? That mirdy this dudgetions?\n\n\n\nKING ARINIA:\n\nI let holt not sucKether,\n\nWhither, efore But lord: I, beget because at that his say\n\nas to brought grave a donesmer all nobe.\n\n\n\nBUCKINGHUMBY:\n\nWhich forgeled! Came; nor thereforn's fiends strefet.\n\n\n\nPLORIA:\n\nYet to Capprohning, that brird\n\nof say mover a desrick.\n\n\n\nMO\n\nstompars, God the\n\ncitchard is high.\n\n\n\nSeth Second Methere:\n\nMarrmat I unmale the bretcius unfoect that I would back where own thy lurges\n\nAnd, iffillimorture:\n\nAs thou twand, York these that high praut.\n\nPlafe merprates sure dread with her,\n\nAt not your must I suchon? too prant!\n\nO 'hiles clight the bleave is graved before\n"}]},{"cell_type":"code","source":"","metadata":{"id":"PuuyrP90VBnw"},"execution_count":null,"outputs":[]}]}